theme(aspect.ratio = 1) +
labs(title = NULL)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
# Equal Variance
autoplot(best.lm, which = 3, ncol = 1, nrow = 1)
# Influential Points
autoplot(best.lm, which = 4, ncol = 1, nrow = 1)  +
theme(aspect.ratio = 1)
# Multicollinearity
corrplot(cor(best_subset), type = 'upper')
vifs <- vif(best.lm)
mean(vifs)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> filter(Row_ID != 66)
library(tidyverse)
library(ggfortify)
library(car)
library(bestglm)
library(glmnet)
library(GGally)
library(corrplot)
library(cowplot)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> filter(Row_ID != 66)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> slice(-66)
library(tidyverse)
library(ggfortify)
library(car)
library(bestglm)
library(glmnet)
library(GGally)
library(corrplot)
library(cowplot)
# Read in data
raw_dat <- read.csv("C:/Users/18014/Downloads/2024_Post_MM_team_data.csv")
# Filtering out some of the columns that I won't need in order to make it easier to create linear models
filtered_dat <- select(raw_dat, -Team, -Conference, -Quad.1.2.Wins, -Quad.1.2.Losses, -Seed)
# Change the 'Conference Champion' column to a factor instead of 1s and 0s.
raw_dat$Conference.Champ <- as.factor(raw_dat$Conference.Champ)
# Use variable selection to create a smaller model and resolve multicollinearity issues
full_mod <- lm(Quad.1.2.Win.. ~ ., data = filtered_dat)
bestBIC <- bestglm(filtered_dat, IC = 'BIC')
summary(bestBIC$BestModel)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Wins, Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> slice(-66)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Wins + Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = filtered_dat)
View(best_subset)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Wins + Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = filtered_dat)
summary(best.lm)
# New data set
best_subset <- filtered_dat |> select(Quad.1.2.Win.., Win...Away.from.Home, Off.Efficiency, Opp.TO.Rate, Opp.FG.., Opp.3PT.FG.) |> slice(-66)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = filtered_dat)
summary(best.lm)
# Linearity
autoplot(best.lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1) +
labs(title = NULL)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
sort(best.lm$residuals)
# Create new model from Best Selection Model
best.lm <- lm(Quad.1.2.Win.. ~ Win...Away.from.Home + Off.Efficiency + Opp.TO.Rate + Opp.FG.. + Opp.3PT.FG., data = best_subset)
summary(best.lm)
# Linearity
autoplot(best.lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1) +
labs(title = NULL)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
# Normality
hist(best.lm$residuals, freq = FALSE, xlab = 'Residuals', main = 'Histogram of Residuals')
shapiro.test(best.lm$residuals)
# Equal Variance
autoplot(best.lm, which = 3, ncol = 1, nrow = 1)
# Influential Points
autoplot(best.lm, which = 4, ncol = 1, nrow = 1)  +
theme(aspect.ratio = 1)
# Multicollinearity
corrplot(cor(best_subset), type = 'upper')
vifs <- vif(best.lm)
mean(vifs)
max(vifs)
# Extracting the p values below .1, or anything that is potentially significant
summary(best.lm)$coefficients[summary(best.lm)$coefficients[, 4] < 0.1, 4]
coef(best.lm)[c("Off.Efficiency", "Opp.TO.Rate")]
```{r, include = FALSE}
summary(bestBIC$BestModel)
## Read in Data
train <- vroom('train.csv')
test <- vroom('test.csv')
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(patchwork)
library(GGally)
library(car)
library(vroom)
bike_pen_reg <- recipe(count ~ ., data = train) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_time(datetime, features = c('hour', 'minute')) |>
step_rm(datetime) |>
step_corr(all_predictors(), threshold = 0.75) |>
step_mutate(season = factor(season)) |>
step_mutate(holiday = factor(holiday)) |>
step_mutate(workingday = factor(workingday)) |>
step_mutate(weather = factor(weather)) |>
step_mutate(datetime_hour = factor(datetime_hour)) |>
step_dummy(all_nominal_predictors) |>
step_normalize(all_numeric_predictors())
## Read in Data
train <- vroom('train.csv')
getwd()
setwd("~/Stat348/KaggleBikeShare")
## Read in Data
train <- vroom('train.csv')
test <- vroom('test.csv')
## Clean Data (Step 1)
train <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
bike_pen_reg <- recipe(count ~ ., data = train) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_time(datetime, features = c('hour', 'minute')) |>
step_rm(datetime) |>
step_corr(all_predictors(), threshold = 0.75) |>
step_mutate(season = factor(season)) |>
step_mutate(holiday = factor(holiday)) |>
step_mutate(workingday = factor(workingday)) |>
step_mutate(weather = factor(weather)) |>
step_mutate(datetime_hour = factor(datetime_hour)) |>
step_dummy(all_nominal_predictors) |>
step_normalize(all_numeric_predictors())
## Workflow
prep_rec <- prep(bike_pen_reg)
bike_pen_reg <- recipe(count ~ ., data = train) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_time(datetime, features = c('hour', 'minute')) |>
step_rm(datetime) |>
step_corr(all_predictors(), threshold = 0.75) |>
step_mutate(season = factor(season)) |>
step_mutate(holiday = factor(holiday)) |>
step_mutate(workingday = factor(workingday)) |>
step_mutate(weather = factor(weather)) |>
step_mutate(datetime_hour = factor(datetime_hour)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_numeric_predictors())
## Workflow
prep_rec <- prep(bike_pen_reg)
new_train <- bake(prep_rec, new_data = train)
pen_mod <- linear_reg(penalty = .5, mixture = 5) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod) |>
fit(data = train)
lin_pred <- exp(predict(workflow, new_data = test))
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod) |>
fit(data = train)
lin_pred <- exp(predict(workflow, new_data = test))
## Read in Data
train <- vroom('train.csv')
test <- vroom('test.csv')
## Clean Data (Step 1)
train <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
# Recipe setup
bike_pen_reg <- recipe(count ~ ., data = train) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_time(datetime, features = c('hour', 'minute')) |>
step_rm(datetime) |>
step_corr(all_predictors(), threshold = 0.75) |>
step_mutate(season = factor(season)) |>
step_mutate(holiday = factor(holiday)) |>
step_mutate(workingday = factor(workingday)) |>
step_mutate(weather = factor(weather)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_numeric_predictors())
# Workflow setup and model training
pen_mod <- linear_reg(penalty = .5, mixture = 1) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
install.packages('glmnet')
library(glmnet)
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
## Formatting for Kaggle
workflow_pred <- lin_pred |>
bind_cols(test) |>
select(datetime, .pred) |>
rename(count = .pred) |>
mutate(datetime = as.character(format(datetime)))
vroom_write(x=workflow_pred, file = "./PenalizedPred.csv", delim=",")
# Workflow setup and model training
pen_mod <- linear_reg(penalty = .05, mixture = 1) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
## Formatting for Kaggle
workflow_pred <- lin_pred |>
bind_cols(test) |>
select(datetime, .pred) |>
rename(count = .pred) |>
mutate(datetime = as.character(format(datetime)))
vroom_write(x=workflow_pred, file = "./PenalizedPred.csv", delim=",")
# Workflow setup and model training
pen_mod <- linear_reg(penalty = .05, mixture = 5) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
## Formatting for Kaggle
workflow_pred <- lin_pred |>
bind_cols(test) |>
select(datetime, .pred) |>
rename(count = .pred) |>
mutate(datetime = as.character(format(datetime)))
vroom_write(x=workflow_pred, file = "./PenalizedPred.csv", delim=",")
# Workflow setup and model training
pen_mod <- linear_reg(penalty = .05, mixture = 10) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Workflow setup and model training
pen_mod <- linear_reg(penalty = .05, mixture = .5) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
## Formatting for Kaggle
workflow_pred <- lin_pred |>
bind_cols(test) |>
select(datetime, .pred) |>
rename(count = .pred) |>
mutate(datetime = as.character(format(datetime)))
vroom_write(x=workflow_pred, file = "./PenalizedPred.csv", delim=",")
# Workflow setup and model training
pen_mod <- linear_reg(penalty = .01, mixture = .1) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
## Formatting for Kaggle
workflow_pred <- lin_pred |>
bind_cols(test) |>
select(datetime, .pred) |>
rename(count = .pred) |>
mutate(datetime = as.character(format(datetime)))
vroom_write(x=workflow_pred, file = "./PenalizedPred.csv", delim=",")
# Workflow setup and model training
pen_mod <- linear_reg(penalty = .01, mixture = .9) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
## Formatting for Kaggle
workflow_pred <- lin_pred |>
bind_cols(test) |>
select(datetime, .pred) |>
rename(count = .pred) |>
mutate(datetime = as.character(format(datetime)))
vroom_write(x=workflow_pred, file = "./PenalizedPred.csv", delim=",")
# Workflow setup and model training
pen_mod <- linear_reg(penalty = 10, mixture = .9) |>
set_engine("glmnet") |>
set_mode("regression")
workflow <- workflow() |>
add_recipe(bike_pen_reg) |>
add_model(pen_mod)
# Fit the model to the training data
fitted_workflow <- fit(workflow, data = train)
# Make predictions on the test set
lin_pred <- exp(predict(fitted_workflow, new_data = test))
## Formatting for Kaggle
workflow_pred <- lin_pred |>
bind_cols(test) |>
select(datetime, .pred) |>
rename(count = .pred) |>
mutate(datetime = as.character(format(datetime)))
vroom_write(x=workflow_pred, file = "./PenalizedPred.csv", delim=",")
library(poissonreg)
library(tidyverse)
library(tidymodels)
library(vroom)
library(glmnet)
## Read in Data
train <- vroom('train.csv')
test <- vroom('test.csv')
## Clean Data (Step 1)
train <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
## Workflow
bike_rec <- recipe(count ~ ., data = train) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_time(datetime, features = c('hour', 'minute')) |>
step_rm(datetime) |>
step_corr(all_predictors(), threshold = 0.75) |>
step_mutate(season = factor(season)) |>
step_mutate(holiday = factor(holiday)) |>
step_mutate(workingday = factor(workingday)) |>
step_mutate(weather = factor(weather)) |>
step_mutate(datetime_hour = factor(datetime_hour))
prep_rec <- prep(bike_rec)
new_train <- bake(prep_rec, new_data = train)
## Penalized Regression Model
p_reg_mod <- linear_reg(penalty = tune(),
mixture = tune()) |>
set_engine('glmnet')
## Workflow
workflow <- workflow() |>
add_recipe(bike_rec) |>
add_model(p_reg_mod)
## Grid of values to tune
grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Cross Validation split
fold <- vfold_cv(train, v = 5, repeats = 1)
## Run CV
CV <- workflow |>
tune_grid(resamples = fold, grid = grid, metrics = metric_Set(rmse, mse, rsq))
## Run CV
CV <- workflow |>
tune_grid(resamples = fold, grid = grid, metrics = metric_set(rmse, mse, rsq))
## Run CV
CV <- workflow |>
tune_grid(resamples = fold, grid = grid, metrics = metric_set(rmse, mae, rsq))
corrplot(new_train)
install.packages('corrplot')
library(corrplot)
## Run CV
CV <- workflow |>
tune_grid(resamples = fold, grid = grid, metrics = metric_set(rmse, mae, rsq))
corrplot(new_train)
## Read in Data
train <- vroom('train.csv')
test <- vroom('test.csv')
## Clean Data (Step 1)
train <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
## Workflow
bike_rec <- recipe(count ~ ., data = train) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_time(datetime, features = c('hour')) |>
step_rm(datetime) |>
step_corr(all_predictors(), threshold = 0.75) |>
step_mutate(season = factor(season)) |>
step_mutate(holiday = factor(holiday)) |>
step_mutate(workingday = factor(workingday)) |>
step_mutate(weather = factor(weather)) |>
step_mutate(datetime_hour = factor(datetime_hour))
prep_rec <- prep(bike_rec)
new_train <- bake(prep_rec, new_data = train)
## Penalized Regression Model
p_reg_mod <- linear_reg(penalty = tune(),
mixture = tune()) |>
set_engine('glmnet')
## Workflow
workflow <- workflow() |>
add_recipe(bike_rec) |>
add_model(p_reg_mod)
## Grid of values to tune
grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Cross Validation split
fold <- vfold_cv(train, v = 5, repeats = 1)
## Run CV
CV <- workflow |>
tune_grid(resamples = fold, grid = grid, metrics = metric_set(rmse, mae, rsq))
## Read in Data
train <- vroom('train.csv')
test <- vroom('test.csv')
## Clean Data (Step 1)
train <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
## Workflow
bike_rec <- recipe(count ~ ., data = train) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_time(datetime, features = c('hour', 'minute')) |>
step_rm(datetime) |>
step_corr(all_predictors(), threshold = 0.75) |>
step_mutate(season = factor(season)) |>
step_mutate(holiday = factor(holiday)) |>
step_mutate(workingday = factor(workingday)) |>
step_mutate(weather = factor(weather)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_numeric_predictors())
prep_rec <- prep(bike_rec)
## Workflow
bike_rec <- recipe(count ~ ., data = train) |>
step_mutate(weather = ifelse(weather == 4, 3, weather)) |>
step_time(datetime, features = c('hour')) |>
step_rm(datetime) |>
step_corr(all_predictors(), threshold = 0.75) |>
step_mutate(season = factor(season)) |>
step_mutate(holiday = factor(holiday)) |>
step_mutate(workingday = factor(workingday)) |>
step_mutate(weather = factor(weather)) |>
step_dummy(all_nominal_predictors()) |>
step_normalize(all_numeric_predictors())
prep_rec <- prep(bike_rec)
new_train <- bake(prep_rec, new_data = train)
## Penalized Regression Model
p_reg_mod <- linear_reg(penalty = tune(),
mixture = tune()) |>
set_engine('glmnet')
## Workflow
workflow <- workflow() |>
add_recipe(bike_rec) |>
add_model(p_reg_mod)
## Grid of values to tune
grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Cross Validation split
fold <- vfold_cv(train, v = 5, repeats = 1)
## Run CV
CV <- workflow |>
tune_grid(resamples = fold, grid = grid, metrics = metric_set(rmse, mae, rsq))
## Plot Results
collect_metrics(CV) |>
filter(.metric=='rmse') |>
ggplot(data=., aes(x=penalty, y= mean, color = factor(mixture))) +
geom_line()
## Plot Results
collect_metrics(CV) |>
filter(.metric=='rmse') |>
ggplot(aes(x=penalty, y= mean, color = factor(mixture))) +
geom_line()
## Find Best tuning parameters
best <- CV |> select_best(metric = 'rmse')
## Finalize Workflow
final_wf <- workflow |>
finalize_workflow(best) |>
fit(data = train)
## Predict
final_wf |>
predict(new_data = test)
## Predict
lin_pred <- final_wf |>
predict(new_data = test)
## Format
workflow_pred <- lin_pred |>
bind_cols(test) |>
select(datetime, .pred) |>
rename(count = .pred) |>
mutate(datetime = as.character(format(datetime)))
vroom_write(x=workflow_pred, file = "./PenalizedPred.csv", delim=",")
vroom_write(x=workflow_pred, file = "./TuningPred.csv", delim=",")
